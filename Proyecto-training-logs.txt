[farojos@evilab-pc py3Proyecto]$ python train.py                                              
Using TensorFlow backend.                                                                                                                                                                                                                                                             
2018-07-03 14:35:08.120248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero                                                           
2018-07-03 14:35:08.120674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties:                                                                                                                                                                  
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6575                                                                                                                                                                                                              
pciBusID: 0000:01:00.0                                                                                                                                                                                                                                                                
totalMemory: 10.91GiB freeMemory: 10.76GiB                                                                                                                                                                                                                                            
2018-07-03 14:35:08.120698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0                                                                                                                                                                    
2018-07-03 14:35:08.289259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-03 14:35:08.289300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0
2018-07-03 14:35:08.289308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N
2018-07-03 14:35:08.289454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10414 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capabilit$
: 6.1)
WARNING:tensorflow:From /home/farojos/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1192: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /home/farojos/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1315: calling reduce_any (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /home/farojos/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1210: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /home/farojos/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1156: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
Creating the model...Done!
WARNING:tensorflow:From /home/farojos/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
Compiling Keras model...Done!
Loading datasets...Done!
Preparing generators...Done!
Training...Epoch 1/60
434/435 [============================>.] - ETA: 5s - loss: 8.1923 - answer_start_loss: 4.1951 - answer_end_loss: 3.9972 - answer_start_acc: 0.0727 - answer_end_acc: 0.0812 Epoch 00000: val_loss improved from inf to 7.96789, saving model to models/0-t8.20034540195-v7.9678936637$
.model
435/435 [==============================] - 2368s - loss: 8.2029 - answer_start_loss: 4.2008 - answer_end_loss: 4.0021 - answer_start_acc: 0.0726 - answer_end_acc: 0.0810 - val_loss: 7.9679 - val_answer_start_loss: 4.0343 - val_answer_end_loss: 3.9336 - val_answer_start_acc: 0.$
179 - val_answer_end_acc: 0.1099
Epoch 2/60
434/435 [============================>.] - ETA: 5s - loss: 6.9371 - answer_start_loss: 3.5860 - answer_end_loss: 3.3511 - answer_start_acc: 0.1460 - answer_end_acc: 0.1601 Epoch 00001: val_loss improved from 7.96789 to 7.04201, saving model to models/1-t6.94900634243-v7.042013$
5404.model
435/435 [==============================] - 2351s - loss: 6.9527 - answer_start_loss: 3.5942 - answer_end_loss: 3.3585 - answer_start_acc: 0.1457 - answer_end_acc: 0.1598 - val_loss: 7.0420 - val_answer_start_loss: 3.6657 - val_answer_end_loss: 3.3763 - val_answer_start_acc: 0.$
645 - val_answer_end_acc: 0.1713
Epoch 3/60
434/435 [============================>.] - ETA: 5s - loss: 6.4831 - answer_start_loss: 3.3559 - answer_end_loss: 3.1272 - answer_start_acc: 0.1829 - answer_end_acc: 0.1978 Epoch 00002: val_loss improved from 7.04201 to 6.81588, saving model to models/2-t6.49538393812-v6.815880$
0283.model
435/435 [==============================] - 2347s - loss: 6.4992 - answer_start_loss: 3.3642 - answer_end_loss: 3.1350 - answer_start_acc: 0.1824 - answer_end_acc: 0.1974 - val_loss: 6.8159 - val_answer_start_loss: 3.5408 - val_answer_end_loss: 3.2751 - val_answer_start_acc: 0.$
892 - val_answer_end_acc: 0.2029
Epoch 4/60
434/435 [============================>.] - ETA: 5s - loss: 6.2597 - answer_start_loss: 3.2422 - answer_end_loss: 3.0175 - answer_start_acc: 0.2021 - answer_end_acc: 0.2184 Epoch 00003: val_loss improved from 6.81588 to 6.33053, saving model to models/3-t6.27330709097-v6.330533$
3531.model
435/435 [==============================] - 2346s - loss: 6.2776 - answer_start_loss: 3.2515 - answer_end_loss: 3.0261 - answer_start_acc: 0.2017 - answer_end_acc: 0.2179 - val_loss: 6.3305 - val_answer_start_loss: 3.2743 - val_answer_end_loss: 3.0563 - val_answer_start_acc: 0.$
185 - val_answer_end_acc: 0.2296
Epoch 5/60
434/435 [============================>.] - ETA: 5s - loss: 6.0816 - answer_start_loss: 3.1524 - answer_end_loss: 2.9292 - answer_start_acc: 0.2189 - answer_end_acc: 0.2355 Epoch 00004: val_loss improved from 6.33053 to 6.14215, saving model to models/4-t6.0954997216-v6.1421459$
836.model
435/435 [==============================] - 2347s - loss: 6.0998 - answer_start_loss: 3.1618 - answer_end_loss: 2.9380 - answer_start_acc: 0.2184 - answer_end_acc: 0.2350 - val_loss: 6.1421 - val_answer_start_loss: 3.2040 - val_answer_end_loss: 2.9381 - val_answer_start_acc: 0.$
254 - val_answer_end_acc: 0.2413
Epoch 6/60
434/435 [============================>.] - ETA: 5s - loss: 5.9188 - answer_start_loss: 3.0687 - answer_end_loss: 2.8502 - answer_start_acc: 0.2325 - answer_end_acc: 0.2508 Epoch 00005: val_loss improved from 6.14215 to 6.07991, saving model to models/5-t5.93333760675-v6.0799086
2726.model
435/435 [==============================] - 2350s - loss: 5.9379 - answer_start_loss: 3.0785 - answer_end_loss: 2.8594 - answer_start_acc: 0.2320 - answer_end_acc: 0.2502 - val_loss: 6.0799 - val_answer_start_loss: 3.1331 - val_answer_end_loss: 2.9468 - val_answer_start_acc: 0.2
396 - val_answer_end_acc: 0.2554
Epoch 7/60
434/435 [============================>.] - ETA: 5s - loss: 5.7701 - answer_start_loss: 2.9923 - answer_end_loss: 2.7778 - answer_start_acc: 0.2486 - answer_end_acc: 0.2664 Epoch 00006: val_loss improved from 6.07991 to 5.84116, saving model to models/6-t5.78530130031-v5.8411607
436.model
435/435 [==============================] - 2354s - loss: 5.7901 - answer_start_loss: 3.0024 - answer_end_loss: 2.7877 - answer_start_acc: 0.2480 - answer_end_acc: 0.2658 - val_loss: 5.8412 - val_answer_start_loss: 3.0186 - val_answer_end_loss: 2.8226 - val_answer_start_acc: 0.2
574 - val_answer_end_acc: 0.2713
Epoch 8/60
434/435 [============================>.] - ETA: 5s - loss: 5.6429 - answer_start_loss: 2.9258 - answer_end_loss: 2.7171 - answer_start_acc: 0.2599 - answer_end_acc: 0.2794 Epoch 00007: val_loss improved from 5.84116 to 5.68614, saving model to models/7-t5.6574000925-v5.68613572
498.model
435/435 [==============================] - 2355s - loss: 5.6619 - answer_start_loss: 2.9358 - answer_end_loss: 2.7261 - answer_start_acc: 0.2593 - answer_end_acc: 0.2789 - val_loss: 5.6861 - val_answer_start_loss: 2.9359 - val_answer_end_loss: 2.7503 - val_answer_start_acc: 0.2
727 - val_answer_end_acc: 0.2872
Epoch 9/60
434/435 [============================>.] - ETA: 5s - loss: 5.5342 - answer_start_loss: 2.8688 - answer_end_loss: 2.6654 - answer_start_acc: 0.2697 - answer_end_acc: 0.2892 Epoch 00008: val_loss did not improve
435/435 [==============================] - 2353s - loss: 5.5524 - answer_start_loss: 2.8785 - answer_end_loss: 2.6740 - answer_start_acc: 0.2691 - answer_end_acc: 0.2886 - val_loss: 5.7234 - val_answer_start_loss: 2.9294 - val_answer_end_loss: 2.7940 - val_answer_start_acc: 0.2
805 - val_answer_end_acc: 0.2984
Epoch 10/60
434/435 [============================>.] - ETA: 5s - loss: 5.4460 - answer_start_loss: 2.8229 - answer_end_loss: 2.6231 - answer_start_acc: 0.2802 - answer_end_acc: 0.2991 Epoch 00009: val_loss improved from 5.68614 to 5.60072, saving model to models/9-t5.46199504647-v5.6007198
9096.model
435/435 [==============================] - 2354s - loss: 5.4670 - answer_start_loss: 2.8339 - answer_end_loss: 2.6330 - answer_start_acc: 0.2796 - answer_end_acc: 0.2984 - val_loss: 5.6007 - val_answer_start_loss: 2.8791 - val_answer_end_loss: 2.7216 - val_answer_start_acc: 0.2
818 - val_answer_end_acc: 0.2983
Epoch 11/60
434/435 [============================>.] - ETA: 5s - loss: 5.3512 - answer_start_loss: 2.7762 - answer_end_loss: 2.5750 - answer_start_acc: 0.2885 - answer_end_acc: 0.3109 Epoch 00010: val_loss did not improve
435/435 [==============================] - 2346s - loss: 5.3704 - answer_start_loss: 2.7863 - answer_end_loss: 2.5841 - answer_start_acc: 0.2878 - answer_end_acc: 0.3103 - val_loss: 5.6832 - val_answer_start_loss: 2.8930 - val_answer_end_loss: 2.7903 - val_answer_start_acc: 0.2
854 - val_answer_end_acc: 0.3002
Epoch 12/60
434/435 [============================>.] - ETA: 5s - loss: 5.2787 - answer_start_loss: 2.7379 - answer_end_loss: 2.5408 - answer_start_acc: 0.2978 - answer_end_acc: 0.3162 Epoch 00011: val_loss did not improve
435/435 [==============================] - 2353s - loss: 5.2969 - answer_start_loss: 2.7474 - answer_end_loss: 2.5495 - answer_start_acc: 0.2972 - answer_end_acc: 0.3155 - val_loss: 5.6941 - val_answer_start_loss: 2.8757 - val_answer_end_loss: 2.8184 - val_answer_start_acc: 0.3
058 - val_answer_end_acc: 0.3183
Epoch 13/60
434/435 [============================>.] - ETA: 5s - loss: 5.2012 - answer_start_loss: 2.6999 - answer_end_loss: 2.5013 - answer_start_acc: 0.3068 - answer_end_acc: 0.3270 Epoch 00012: val_loss improved from 5.60072 to 5.43894, saving model to models/12-t5.21649681589-v5.438936
05177.model
435/435 [==============================] - 2351s - loss: 5.2213 - answer_start_loss: 2.7104 - answer_end_loss: 2.5109 - answer_start_acc: 0.3062 - answer_end_acc: 0.3263 - val_loss: 5.4389 - val_answer_start_loss: 2.7893 - val_answer_end_loss: 2.6496 - val_answer_start_acc: 0.3
061 - val_answer_end_acc: 0.3344
Epoch 14/60
434/435 [============================>.] - ETA: 5s - loss: 5.1311 - answer_start_loss: 2.6641 - answer_end_loss: 2.4670 - answer_start_acc: 0.3109 - answer_end_acc: 0.3322 Epoch 00013: val_loss improved from 5.43894 to 5.23122, saving model to models/13-t5.14676449201-v5.231221
73407.model
435/435 [==============================] - 2352s - loss: 5.1517 - answer_start_loss: 2.6747 - answer_end_loss: 2.4770 - answer_start_acc: 0.3102 - answer_end_acc: 0.3315 - val_loss: 5.2312 - val_answer_start_loss: 2.6996 - val_answer_end_loss: 2.5316 - val_answer_start_acc: 0.3
147 - val_answer_end_acc: 0.3440
Epoch 15/60
434/435 [============================>.] - ETA: 5s - loss: 5.0762 - answer_start_loss: 2.6354 - answer_end_loss: 2.4407 - answer_start_acc: 0.3161 - answer_end_acc: 0.3383 Epoch 00014: val_loss improved from 5.23122 to 5.17522, saving model to models/14-t5.09154658256-v5.175221
05211.model
435/435 [==============================] - 2355s - loss: 5.0964 - answer_start_loss: 2.6459 - answer_end_loss: 2.4505 - answer_start_acc: 0.3154 - answer_end_acc: 0.3376 - val_loss: 5.1752 - val_answer_start_loss: 2.6697 - val_answer_end_loss: 2.5056 - val_answer_start_acc: 0.3
297 - val_answer_end_acc: 0.3525
Epoch 16/60
434/435 [============================>.] - ETA: 5s - loss: 5.0115 - answer_start_loss: 2.6019 - answer_end_loss: 2.4096 - answer_start_acc: 0.3244 - answer_end_acc: 0.3450 Epoch 00015: val_loss improved from 5.17522 to 5.07076, saving model to models/15-t5.02636322245-v5.070757
33189.model
435/435 [==============================] - 2349s - loss: 5.0310 - answer_start_loss: 2.6121 - answer_end_loss: 2.4189 - answer_start_acc: 0.3236 - answer_end_acc: 0.3444 - val_loss: 5.0708 - val_answer_start_loss: 2.6325 - val_answer_end_loss: 2.4382 - val_answer_start_acc: 0.3
325 - val_answer_end_acc: 0.3532
Epoch 17/60
434/435 [============================>.] - ETA: 5s - loss: 4.9408 - answer_start_loss: 2.5659 - answer_end_loss: 2.3749 - answer_start_acc: 0.3332 - answer_end_acc: 0.3526 Epoch 00016: val_loss did not improve
435/435 [==============================] - 2352s - loss: 4.9616 - answer_start_loss: 2.5765 - answer_end_loss: 2.3851 - answer_start_acc: 0.3325 - answer_end_acc: 0.3519 - val_loss: 5.2059 - val_answer_start_loss: 2.6991 - val_answer_end_loss: 2.5068 - val_answer_start_acc: 0.3
203 - val_answer_end_acc: 0.3481
Epoch 18/60
434/435 [============================>.] - ETA: 5s - loss: 4.8971 - answer_start_loss: 2.5438 - answer_end_loss: 2.3533 - answer_start_acc: 0.3373 - answer_end_acc: 0.3585 Epoch 00017: val_loss improved from 5.07076 to 5.06258, saving model to models/17-t4.91326835877-v5.062584
70037.model
435/435 [==============================] - 2352s - loss: 4.9183 - answer_start_loss: 2.5545 - answer_end_loss: 2.3639 - answer_start_acc: 0.3365 - answer_end_acc: 0.3578 - val_loss: 5.0626 - val_answer_start_loss: 2.6359 - val_answer_end_loss: 2.4267 - val_answer_start_acc: 0.3
300 - val_answer_end_acc: 0.3596
Epoch 19/60                                                                                                                                                                                                                                                                        
434/435 [============================>.] - ETA: 5s - loss: 4.8331 - answer_start_loss: 2.5126 - answer_end_loss: 2.3205 - answer_start_acc: 0.3444 - answer_end_acc: 0.3675 Epoch 00018: val_loss improved from 5.06258 to 5.02915, saving model to models/18-t4.84648987888-v5.02915$
8777.model
435/435 [==============================] - 2597s - loss: 4.8507 - answer_start_loss: 2.5215 - answer_end_loss: 2.3292 - answer_start_acc: 0.3437 - answer_end_acc: 0.3669 - val_loss: 5.0292 - val_answer_start_loss: 2.6065 - val_answer_end_loss: 2.4226 - val_answer_start_acc: 0.$
473 - val_answer_end_acc: 0.3742
Epoch 20/60
434/435 [============================>.] - ETA: 5s - loss: 4.7855 - answer_start_loss: 2.4869 - answer_end_loss: 2.2986 - answer_start_acc: 0.3501 - answer_end_acc: 0.3741 Epoch 00019: val_loss improved from 5.02915 to 4.92864, saving model to models/19-t4.79938763675-v4.92864$
7342.model
435/435 [==============================] - 2570s - loss: 4.8037 - answer_start_loss: 2.4962 - answer_end_loss: 2.3076 - answer_start_acc: 0.3494 - answer_end_acc: 0.3736 - val_loss: 4.9286 - val_answer_start_loss: 2.5538 - val_answer_end_loss: 2.3749 - val_answer_start_acc: 0.$
459 - val_answer_end_acc: 0.3842
Epoch 21/60
434/435 [============================>.] - ETA: 5s - loss: 4.7300 - answer_start_loss: 2.4593 - answer_end_loss: 2.2707 - answer_start_acc: 0.3552 - answer_end_acc: 0.3806 Epoch 00020: val_loss improved from 4.92864 to 4.83377, saving model to models/20-t4.74462540499-v4.83376$
79522.model
435/435 [==============================] - 2349s - loss: 4.7492 - answer_start_loss: 2.4691 - answer_end_loss: 2.2801 - answer_start_acc: 0.3544 - answer_end_acc: 0.3800 - val_loss: 4.8338 - val_answer_start_loss: 2.5068 - val_answer_end_loss: 2.3269 - val_answer_start_acc: 0.$
514 - val_answer_end_acc: 0.3820
Epoch 22/60
434/435 [============================>.] - ETA: 5s - loss: 4.6677 - answer_start_loss: 2.4255 - answer_end_loss: 2.2422 - answer_start_acc: 0.3607 - answer_end_acc: 0.3846 Epoch 00021: val_loss improved from 4.83377 to 4.83334, saving model to models/21-t4.68161649248-v4.83334$
21077.model
435/435 [==============================] - 2410s - loss: 4.6860 - answer_start_loss: 2.4348 - answer_end_loss: 2.2512 - answer_start_acc: 0.3600 - answer_end_acc: 0.3840 - val_loss: 4.8333 - val_answer_start_loss: 2.5075 - val_answer_end_loss: 2.3259 - val_answer_start_acc: 0.$
581 - val_answer_end_acc: 0.3868
Epoch 23/60
434/435 [============================>.] - ETA: 6s - loss: 4.6182 - answer_start_loss: 2.4006 - answer_end_loss: 2.2176 - answer_start_acc: 0.3669 - answer_end_acc: 0.3915 Epoch 00022: val_loss improved from 4.83334 to 4.75248, saving model to models/22-t4.63113701343-v4.75248$
95439.model
435/435 [==============================] - 2683s - loss: 4.6352 - answer_start_loss: 2.4094 - answer_end_loss: 2.2258 - answer_start_acc: 0.3662 - answer_end_acc: 0.3910 - val_loss: 4.7525 - val_answer_start_loss: 2.4786 - val_answer_end_loss: 2.2739 - val_answer_start_acc: 0.3
638 - val_answer_end_acc: 0.3894
Epoch 24/60
434/435 [============================>.] - ETA: 6s - loss: 4.5612 - answer_start_loss: 2.3717 - answer_end_loss: 2.1895 - answer_start_acc: 0.3741 - answer_end_acc: 0.3971 Epoch 00023: val_loss improved from 4.75248 to 4.62942, saving model to models/23-t4.57414154979-v4.629416
526.model
435/435 [==============================] - 2789s - loss: 4.5782 - answer_start_loss: 2.3803 - answer_end_loss: 2.1979 - answer_start_acc: 0.3735 - answer_end_acc: 0.3965 - val_loss: 4.6294 - val_answer_start_loss: 2.3977 - val_answer_end_loss: 2.2317 - val_answer_start_acc: 0.3
688 - val_answer_end_acc: 0.3946
Epoch 25/60
434/435 [============================>.] - ETA: 5s - loss: 4.5031 - answer_start_loss: 2.3423 - answer_end_loss: 2.1608 - answer_start_acc: 0.3801 - answer_end_acc: 0.4026 Epoch 00024: val_loss did not improve
435/435 [==============================] - 2350s - loss: 4.5184 - answer_start_loss: 2.3502 - answer_end_loss: 2.1682 - answer_start_acc: 0.3795 - answer_end_acc: 0.4021 - val_loss: 4.6492 - val_answer_start_loss: 2.4148 - val_answer_end_loss: 2.2344 - val_answer_start_acc: 0.3
739 - val_answer_end_acc: 0.4055
Epoch 26/60
434/435 [============================>.] - ETA: 5s - loss: 4.4470 - answer_start_loss: 2.3170 - answer_end_loss: 2.1300 - answer_start_acc: 0.3850 - answer_end_acc: 0.4102 Epoch 00025: val_loss did not improve
435/435 [==============================] - 2349s - loss: 4.4617 - answer_start_loss: 2.3247 - answer_end_loss: 2.1370 - answer_start_acc: 0.3844 - answer_end_acc: 0.4098 - val_loss: 4.6584 - val_answer_start_loss: 2.4068 - val_answer_end_loss: 2.2515 - val_answer_start_acc: 0.3
789 - val_answer_end_acc: 0.3999
Epoch 27/60
434/435 [============================>.] - ETA: 5s - loss: 4.3899 - answer_start_loss: 2.2884 - answer_end_loss: 2.1014 - answer_start_acc: 0.3909 - answer_end_acc: 0.4180 Epoch 00026: val_loss improved from 4.62942 to 4.57526, saving model to models/26-t4.40072568864-v4.575256
28843.model
435/435 [==============================] - 2351s - loss: 4.4041 - answer_start_loss: 2.2959 - answer_end_loss: 2.1082 - answer_start_acc: 0.3903 - answer_end_acc: 0.4175 - val_loss: 4.5753 - val_answer_start_loss: 2.3748 - val_answer_end_loss: 2.2005 - val_answer_start_acc: 0.3
799 - val_answer_end_acc: 0.4078
Epoch 28/60
434/435 [============================>.] - ETA: 5s - loss: 4.3376 - answer_start_loss: 2.2594 - answer_end_loss: 2.0782 - answer_start_acc: 0.3971 - answer_end_acc: 0.4213 Epoch 00027: val_loss improved from 4.57526 to 4.44712, saving model to models/27-t4.34798517926-v4.447121
13875.model
435/435 [==============================] - 2351s - loss: 4.3512 - answer_start_loss: 2.2666 - answer_end_loss: 2.0847 - answer_start_acc: 0.3965 - answer_end_acc: 0.4208 - val_loss: 4.4471 - val_answer_start_loss: 2.3412 - val_answer_end_loss: 2.1059 - val_answer_start_acc: 0.3
823 - val_answer_end_acc: 0.4211
Epoch 29/60
434/435 [============================>.] - ETA: 5s - loss: 4.2801 - answer_start_loss: 2.2317 - answer_end_loss: 2.0484 - answer_start_acc: 0.4013 - answer_end_acc: 0.4286 Epoch 00028: val_loss improved from 4.44712 to 4.34477, saving model to models/28-t4.29094127623-v4.344771
29228.model
435/435 [==============================] - 2352s - loss: 4.2943 - answer_start_loss: 2.2389 - answer_end_loss: 2.0554 - answer_start_acc: 0.4007 - answer_end_acc: 0.4281 - val_loss: 4.3448 - val_answer_start_loss: 2.2758 - val_answer_end_loss: 2.0690 - val_answer_start_acc: 0.3
892 - val_answer_end_acc: 0.4261
Epoch 30/60
434/435 [============================>.] - ETA: 5s - loss: 4.2358 - answer_start_loss: 2.2092 - answer_end_loss: 2.0266 - answer_start_acc: 0.4087 - answer_end_acc: 0.4366 Epoch 00029: val_loss improved from 4.34477 to 4.31559, saving model to models/29-t4.24676810597-v4.315594
61138.model
435/435 [==============================] - 2348s - loss: 4.2502 - answer_start_loss: 2.2164 - answer_end_loss: 2.0338 - answer_start_acc: 0.4081 - answer_end_acc: 0.4361 - val_loss: 4.3156 - val_answer_start_loss: 2.2636 - val_answer_end_loss: 2.0520 - val_answer_start_acc: 0.3
926 - val_answer_end_acc: 0.4255
434/435 [============================>.] - ETA: 5s - loss: 4.2061 - answer_start_loss: 2.1921 - answer_end_loss: 2.0140 - answer_start_acc: 0.4089 - answer_end_acc: 0.4365 Epoch 00030: val_loss improved from 4.31559 to 4.29287, saving model to models/30-t4.21699486005-v4.292865
92522.model
435/435 [==============================] - 2353s - loss: 4.2204 - answer_start_loss: 2.1993 - answer_end_loss: 2.0211 - answer_start_acc: 0.4083 - answer_end_acc: 0.4360 - val_loss: 4.2929 - val_answer_start_loss: 2.2606 - val_answer_end_loss: 2.0323 - val_answer_start_acc: 0.3
955 - val_answer_end_acc: 0.4300
Epoch 32/60
434/435 [============================>.] - ETA: 6s - loss: 4.1557 - answer_start_loss: 2.1656 - answer_end_loss: 1.9902 - answer_start_acc: 0.4194 - answer_end_acc: 0.4450 Epoch 00031: val_loss improved from 4.29287 to 4.27977, saving model to models/31-t4.16619682675-v4.279771
02114.model
435/435 [==============================] - 2748s - loss: 4.1695 - answer_start_loss: 2.1723 - answer_end_loss: 1.9971 - answer_start_acc: 0.4189 - answer_end_acc: 0.4445 - val_loss: 4.2798 - val_answer_start_loss: 2.2442 - val_answer_end_loss: 2.0355 - val_answer_start_acc: 0.3
969 - val_answer_end_acc: 0.4297
Epoch 33/60
434/435 [============================>.] - ETA: 5s - loss: 4.1254 - answer_start_loss: 2.1496 - answer_end_loss: 1.9758 - answer_start_acc: 0.4203 - answer_end_acc: 0.4456 Epoch 00032: val_loss improved from 4.27977 to 4.27378, saving model to models/32-t4.13574232327-v4.273775
57908.model
435/435 [==============================] - 2352s - loss: 4.1390 - answer_start_loss: 2.1563 - answer_end_loss: 1.9827 - answer_start_acc: 0.4198 - answer_end_acc: 0.4451 - val_loss: 4.2738 - val_answer_start_loss: 2.2390 - val_answer_end_loss: 2.0348 - val_answer_start_acc: 0.4
032 - val_answer_end_acc: 0.4412
Epoch 34/60
434/435 [============================>.] - ETA: 5s - loss: 4.0794 - answer_start_loss: 2.1260 - answer_end_loss: 1.9535 - answer_start_acc: 0.4254 - answer_end_acc: 0.4518 Epoch 00033: val_loss improved from 4.27378 to 4.23276, saving model to models/33-t4.09084894471-v4.232756
83472.model
435/435 [==============================] - 2357s - loss: 4.0944 - answer_start_loss: 2.1333 - answer_end_loss: 1.9611 - answer_start_acc: 0.4248 - answer_end_acc: 0.4512 - val_loss: 4.2328 - val_answer_start_loss: 2.2226 - val_answer_end_loss: 2.0101 - val_answer_start_acc: 0.4
054 - val_answer_end_acc: 0.4379
Epoch 35/60
434/435 [============================>.] - ETA: 5s - loss: 4.0402 - answer_start_loss: 2.1057 - answer_end_loss: 1.9344 - answer_start_acc: 0.4301 - answer_end_acc: 0.4560 Epoch 00034: val_loss improved from 4.23276 to 4.11349, saving model to models/34-t4.0509029594-v4.1134907
5918.model
435/435 [==============================] - 2350s - loss: 4.0543 - answer_start_loss: 2.1127 - answer_end_loss: 1.9416 - answer_start_acc: 0.4295 - answer_end_acc: 0.4555 - val_loss: 4.1135 - val_answer_start_loss: 2.1621 - val_answer_end_loss: 1.9514 - val_answer_start_acc: 0.4
170 - val_answer_end_acc: 0.4511
Epoch 36/60
434/435 [============================>.] - ETA: 5s - loss: 4.0051 - answer_start_loss: 2.0896 - answer_end_loss: 1.9154 - answer_start_acc: 0.4348 - answer_end_acc: 0.4623 Epoch 00035: val_loss did not improve
435/435 [==============================] - 2349s - loss: 4.0195 - answer_start_loss: 2.0967 - answer_end_loss: 1.9228 - answer_start_acc: 0.4342 - answer_end_acc: 0.4616 - val_loss: 4.2172 - val_answer_start_loss: 2.2159 - val_answer_end_loss: 2.0013 - val_answer_start_acc: 0.4
070 - val_answer_end_acc: 0.4485
Epoch 37/60
434/435 [============================>.] - ETA: 5s - loss: 3.9552 - answer_start_loss: 2.0621 - answer_end_loss: 1.8931 - answer_start_acc: 0.4399 - answer_end_acc: 0.4664 Epoch 00036: val_loss improved from 4.11349 to 4.11207, saving model to models/36-t3.96553647215-v4.112067
339.model
435/435 [==============================] - 2349s - loss: 3.9688 - answer_start_loss: 2.0688 - answer_end_loss: 1.9000 - answer_start_acc: 0.4394 - answer_end_acc: 0.4659 - val_loss: 4.1121 - val_answer_start_loss: 2.1596 - val_answer_end_loss: 1.9525 - val_answer_start_acc: 0.4
186 - val_answer_end_acc: 0.4593
Epoch 38/60
434/435 [============================>.] - ETA: 5s - loss: 3.9223 - answer_start_loss: 2.0458 - answer_end_loss: 1.8766 - answer_start_acc: 0.4449 - answer_end_acc: 0.4716 Epoch 00037: val_loss improved from 4.11207 to 4.10710, saving model to models/37-t3.9333260908-v4.1070989
5414.model
435/435 [==============================] - 2350s - loss: 3.9368 - answer_start_loss: 2.0529 - answer_end_loss: 1.8839 - answer_start_acc: 0.4442 - answer_end_acc: 0.4710 - val_loss: 4.1071 - val_answer_start_loss: 2.1514 - val_answer_end_loss: 1.9557 - val_answer_start_acc: 0.4
212 - val_answer_end_acc: 0.4585
Epoch 39/60
434/435 [============================>.] - ETA: 5s - loss: 3.8856 - answer_start_loss: 2.0249 - answer_end_loss: 1.8607 - answer_start_acc: 0.4499 - answer_end_acc: 0.4766 Epoch 00038: val_loss improved from 4.10710 to 4.08198, saving model to models/38-t3.89651707282-v4.081983
8578.model
435/435 [==============================] - 2351s - loss: 3.8999 - answer_start_loss: 2.0319 - answer_end_loss: 1.8680 - answer_start_acc: 0.4493 - answer_end_acc: 0.4759 - val_loss: 4.0820 - val_answer_start_loss: 2.1542 - val_answer_end_loss: 1.9277 - val_answer_start_acc: 0.4
254 - val_answer_end_acc: 0.4680
Epoch 40/60
434/435 [============================>.] - ETA: 5s - loss: 3.8519 - answer_start_loss: 2.0103 - answer_end_loss: 1.8416 - answer_start_acc: 0.4552 - answer_end_acc: 0.4834 Epoch 00039: val_loss improved from 4.08198 to 4.01627, saving model to models/39-t3.86255569269-v4.016267
26494.model
435/435 [==============================] - 2352s - loss: 3.8659 - answer_start_loss: 2.0172 - answer_end_loss: 1.8487 - answer_start_acc: 0.4546 - answer_end_acc: 0.4829 - val_loss: 4.0163 - val_answer_start_loss: 2.1075 - val_answer_end_loss: 1.9087 - val_answer_start_acc: 0.4
304 - val_answer_end_acc: 0.4649
Epoch 41/60
434/435 [============================>.] - ETA: 5s - loss: 3.8138 - answer_start_loss: 1.9898 - answer_end_loss: 1.8240 - answer_start_acc: 0.4574 - answer_end_acc: 0.4856 Epoch 00040: val_loss did not improve
435/435 [==============================] - 2350s - loss: 3.8291 - answer_start_loss: 1.9973 - answer_end_loss: 1.8318 - answer_start_acc: 0.4567 - answer_end_acc: 0.4848 - val_loss: 4.0531 - val_answer_start_loss: 2.1354 - val_answer_end_loss: 1.9177 - val_answer_start_acc: 0.4
284 - val_answer_end_acc: 0.4724
Epoch 42/60
434/435 [============================>.] - ETA: 5s - loss: 3.7746 - answer_start_loss: 1.9691 - answer_end_loss: 1.8055 - answer_start_acc: 0.4617 - answer_end_acc: 0.4905 Epoch 00041: val_loss improved from 4.01627 to 4.00792, saving model to models/41-t3.78587505098-v4.007915
17162.model
435/435 [==============================] - 2352s - loss: 3.7894 - answer_start_loss: 1.9763 - answer_end_loss: 1.8130 - answer_start_acc: 0.4610 - answer_end_acc: 0.4898 - val_loss: 4.0079 - val_answer_start_loss: 2.1063 - val_answer_end_loss: 1.9017 - val_answer_start_acc: 0.4
340 - val_answer_end_acc: 0.4732
Epoch 43/60
 83/435 [====>.........................] - ETA: 1862s - loss: 3.7323 - answer_start_loss: 1.9465 - answer_end_loss: 1.7857 - answer_start_acc: 0.4683 - answer_end_acc: 0.5022